

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Lexicon &mdash; XRL  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="License" href="license.html" />
    <link rel="prev" title="PPO" href="algo/ppo.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> XRL
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Bibliographie:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="biblio/visu_understanding_atari.html">Visualizing and Understanding Atari Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="biblio/notebook/Overview_Visualizing_Atari_Agents.html">Overview - Visualizing and Understanding Atari Agents</a></li>
</ul>
<p class="caption"><span class="caption-text">General Knowledge:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="rl/introduction.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="rl/dqn_intro.html">Deep Q Network</a></li>
</ul>
<p class="caption"><span class="caption-text">Algorithms:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="algo/dqn.html">DQN</a></li>
<li class="toctree-l1"><a class="reference internal" href="algo/ppo.html">PPO</a></li>
</ul>
<p class="caption"><span class="caption-text">Annexes:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Lexicon</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#actor-critic">Actor-Critic</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cross-entropy-loss-function">Cross-Entropy (Loss Function)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#distillation">Distillation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#exploration-vs-exploitation-trade-off">Exploration vs Exploitation trade-off</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-free-vs-model-based">Model-Free vs Model-Based</a></li>
<li class="toctree-l2"><a class="reference internal" href="#motor-learning">Motor Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#off-policy-vs-on-policy">Off Policy vs On Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#policy-based-vs-value-based">Policy-based vs Value-based</a></li>
<li class="toctree-l2"><a class="reference internal" href="#replay-buffer-memory">Replay Buffer / Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tabular-q-learning">Tabular Q-learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#target-clipping">Target Clipping</a></li>
<li class="toctree-l2"><a class="reference internal" href="#target-network">Target Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="#value-iteration">Value Iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#atari-wrappers">(Atari) Wrappers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="help.html">Help</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">XRL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Lexicon</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/lexicon.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="lexicon">
<h1>Lexicon<a class="headerlink" href="#lexicon" title="Permalink to this headline">¶</a></h1>
<div class="section" id="actor-critic">
<h2>Actor-Critic<a class="headerlink" href="#actor-critic" title="Permalink to this headline">¶</a></h2>
<p>When attempting to solve a Reinforcement Learning problem, there are two main methods one can choose from: calculating the Value Functions or Q-Values of each state and choosing actions according to those, or directly compute a policy which defines the probabilities each action should be taken depending on the current state, and act according to it. Actor-Critic algorithms combine the two methods in order to create a more robust method. A great illustrated-comics explanation can be found here.
<a class="reference external" href="https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752">https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752</a></p>
</div>
<div class="section" id="cross-entropy-loss-function">
<h2>Cross-Entropy (Loss Function)<a class="headerlink" href="#cross-entropy-loss-function" title="Permalink to this headline">¶</a></h2>
<p>Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events. It builds upon the idea of entropy from information theory and calculates the number of bits required to represent or transmit an average event from one distribution compared to another distribution.</p>
<p><strong>Intuition</strong> :  if we consider a target or underlying probability distribution <span class="math notranslate nohighlight">\(P\)</span> and an approximation of the target distribution <span class="math notranslate nohighlight">\(Q\)</span>, then the <em>cross-entropy</em> of <span class="math notranslate nohighlight">\(Q\)</span> from <span class="math notranslate nohighlight">\(P\)</span> is the number of additional bits to represent an event using <span class="math notranslate nohighlight">\(Q\)</span> instead of <span class="math notranslate nohighlight">\(P\)</span> :</p>
<p><span class="math notranslate nohighlight">\(H(P, Q) = - \sum_{x \in X} P(x)\log (Q(x))\)</span></p>
</div>
<div class="section" id="distillation">
<h2>Distillation<a class="headerlink" href="#distillation" title="Permalink to this headline">¶</a></h2>
<p>Method to transfer knowledge from a <em>teacher model T</em> to a <em>student model S</em>.</p>
<p>It is typically used for model compression, in which a small model is trained to mimic a pre-trained model (or ensemble of models). This training setting is referred to as <em>teacher-student</em>.</p>
<p><strong>Paper</strong> : introduced in href{<a class="reference external" href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf">https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf</a>}{<em>Model Compression</em>}, C. Bucila (2006) and generalized in href{<a class="reference external" href="https://arxiv.org/abs/1503.02531">https://arxiv.org/abs/1503.02531</a>}{<em>Distilling the Knowledge in a Neural Network</em>}, C. Hinton. Other materials : href{<a class="reference external" href="https://www.youtube.com/watch?v=EK61htlw8hY">https://www.youtube.com/watch?v=EK61htlw8hY</a>}{video lecture} and href{<a class="reference external" href="https://www.ttic.edu/dl/dark14.pdf">https://www.ttic.edu/dl/dark14.pdf</a>}{slides}.</p>
<p><strong>How does it work</strong> : knowledge is transferred from the teacher model to the student by minimizing a loss function in which the target is the distribution of class probabilities predicted by the teacher model, ie the output of a softmax function on the teacher model’s logits.</p>
<p><strong>Sofmax’s temperature :math:`tau`</strong> : in many cases, the probability distribution has the correct class at a very high probability, with all other class probabilities very close to 0, which doesn’t provide much information beyond the ground truth labels already provided in the dataset. To tackle this issue, Hinton et al., 2015 introduced the concept of “softmax temperature”. With T as the temperature parameter, the probability <span class="math notranslate nohighlight">\(p_i\)</span> of class i is calculated from the logits z as:
<span class="math notranslate nohighlight">\(p_i = \frac{exp(\frac{z_i}{T})}{\sum_j exp(\frac{z_j}{T})}\)</span></p>
<ul class="simple">
<li><p>When T=1 we get the standard softmax function.</p></li>
<li><p>As T grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes the teacher found more similar to the predicted class</p></li>
</ul>
<p>Remarks :</p>
<ul class="simple">
<li><p><em>distillation loss</em> : when computing the loss function vs. the teacher’s soft targets, we use the same value of T to compute the softmax on the student’s logits.</p></li>
<li><p><em>student loss</em> : calculate the “standard” loss between the student’s predicted class probabilities and the ground-truth labels (also called “hard labels/targets”) - and using <span class="math notranslate nohighlight">\(T=1\)</span>.</p></li>
<li><p>The overall loss function, incorporating both distillation and student losses, is calculated as:</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(L(x,W) = \alpha H(y, \sigma(z_s, T=1)) + \beta H(\sigma(z_t, T=\tau), \sigma(z_s, T=\tau))\)</span>, with x the input, W the student model parameter, y the ground truth label, H the cross entropy loss function, <span class="math notranslate nohighlight">\(\sigma\)</span> the softmax function, <span class="math notranslate nohighlight">\(z_s, z_t\)</span> respectively the logits of students and teacher.</p>
<img alt="Images/knowledge_distillation.png" src="Images/knowledge_distillation.png" />
<p>Read more on href{<a class="reference external" href="https://nervanasystems.github.io/distiller/knowledge_distillation.html#bucila-et-al-2006">https://nervanasystems.github.io/distiller/knowledge_distillation.html#bucila-et-al-2006</a>}{Knowledge Distillation
}</p>
</div>
<div class="section" id="exploration-vs-exploitation-trade-off">
<h2>Exploration vs Exploitation trade-off<a class="headerlink" href="#exploration-vs-exploitation-trade-off" title="Permalink to this headline">¶</a></h2>
<p>On the one hand, our agent needs to explore the environment to build a complete picture of transitions and action outcomes. On the other hand, we should use interaction with the environment efficiently: we shouldn’t waste time by randomly trying actions we’ve already tried and have learned their outcomes.</p>
</div>
<div class="section" id="model-free-vs-model-based">
<h2>Model-Free vs Model-Based<a class="headerlink" href="#model-free-vs-model-based" title="Permalink to this headline">¶</a></h2>
<p>The term <strong>model-free</strong> means that the method doesn’t build a model of the environment or reward; it just directly connects observations to actions (or values that are related to actions). In other words, the agent takes current observations and does some computations on them, and the result is the action that it should take.</p>
<p>In contrast, <strong>model-based</strong> methods try to predict what the next
observation and/or reward will be. Based on this prediction, the agent is trying to
choose the best possible action to take, very often making such predictions
multiple times to look more and more steps into the future.</p>
<p>Both classes of methods have strong and weak sides, but usually pure model-based methods are used in deterministic environments, such as board games with
strict rules. On the other hand, model-free methods are usually easier to train as
it’s hard to build good models of complex environments with rich observations.</p>
</div>
<div class="section" id="motor-learning">
<h2>Motor Learning<a class="headerlink" href="#motor-learning" title="Permalink to this headline">¶</a></h2>
<p>often regarded as a process of learning a new mapping from a sensory inputs to motor outputs</p>
</div>
<div class="section" id="off-policy-vs-on-policy">
<h2>Off Policy vs On Policy<a class="headerlink" href="#off-policy-vs-on-policy" title="Permalink to this headline">¶</a></h2>
<p>Every Reinforcement Learning algorithm must follow some policy in order to decide which actions to perform at each state. Still, the learning procedure of the algorithm doesn’t have to take into account that policy while learning. Algorithms which concern about the policy which yielded past state-action decisions are referred to as on-policy algorithms, while those ignoring it are known as off-policy.
A well known off-policy algorithm is Q-Learning, as its update rule uses the action which will yield the highest Q-Value, while the actual policy used might restrict that action or choose another. The on-policy variation of Q-Learning is known as Sarsa, where the update rule uses the action chosen by the followed policy</p>
</div>
<div class="section" id="policy-based-vs-value-based">
<h2>Policy-based vs Value-based<a class="headerlink" href="#policy-based-vs-value-based" title="Permalink to this headline">¶</a></h2>
<p><strong>Policy-based</strong> methods are directly approximating the policy of the agent, that is, what actions the agent should
carry out at every step. Policy is usually represented by probability distribution over the available actions.</p>
<p>In contrast, the method could be <strong>value-based</strong>. In this
case, instead of the probability of actions, the agent calculates the value of every possible action and chooses the action with the best value.</p>
</div>
<div class="section" id="replay-buffer-memory">
<h2>Replay Buffer / Memory<a class="headerlink" href="#replay-buffer-memory" title="Permalink to this headline">¶</a></h2>
<p>It stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure.</p>
</div>
<div class="section" id="tabular-q-learning">
<h2>Tabular Q-learning<a class="headerlink" href="#tabular-q-learning" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Start with an empty table <span class="math notranslate nohighlight">\(Q(s,a)\)</span>, mapping states to values of actions.</p></li>
<li><p>By interacting with the environment, obtain the tuple (s, a, r, s’) (state, action,reward, and the new state). Need to decide which action to take, and there is no single proper way to make this decision (exploration versus exploitation).</p></li>
<li><p>Update the Q(s, a) value using the Bellman approximation update: <span class="math notranslate nohighlight">\(Q_{(s,a)} \longleftarrow r + \gamma \max_{a' \in A} Q_{(s',a')}\)</span></p></li>
<li><p>Repeat from step 2 til convergence.</p></li>
</ul>
<p>Remark, to make the training more stable, use the <em>blending</em> technique which is just averaging between old and new values of Q using learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>. This allows values of Q to converge smoothly, even if our environment is noisy :</p>
<p><span class="math notranslate nohighlight">\(Q_{(s,a)} \longleftarrow (1-\alpha) Q_{(s,a)} + \alpha \left(r + \gamma \max_{a' \in A} Q_{(s',a')} \right)\)</span></p>
</div>
<div class="section" id="target-clipping">
<h2>Target Clipping<a class="headerlink" href="#target-clipping" title="Permalink to this headline">¶</a></h2>
<p>Mitigate target critic overestimation</p>
</div>
<div class="section" id="target-network">
<h2>Target Network<a class="headerlink" href="#target-network" title="Permalink to this headline">¶</a></h2>
<p>The target network has its weights kept frozen most of the time, but is updated with the policy network’s weights every so often. This is usually a set number of steps but we shall use episodes for simplicity.The target network has its weights kept frozen most of the time, but is updated with the policy network’s weights every so often. This is usually a set number of steps but we shall use episodes for simplicity.</p>
</div>
<div class="section" id="value-iteration">
<h2>Value Iteration<a class="headerlink" href="#value-iteration" title="Permalink to this headline">¶</a></h2>
<p>A method that on every step does a loop on all states, and for every state, performs an update of its value with a Bellman approximation.</p>
</div>
<div class="section" id="atari-wrappers">
<h2>(Atari) Wrappers<a class="headerlink" href="#atari-wrappers" title="Permalink to this headline">¶</a></h2>
<p>The list of Atari transformations used by RL researchers includes:</p>
<ul class="simple">
<li><p><strong>Converting individual lives in the game into separate episodes</strong>. In general, an episode contains all the steps from the beginning of the game until the “Game over” screen appears?, which can last for thousands of game steps (observations and actions). Usually, in arcade games, the player is given several lives, which provide several attempts in the game. This transformation <strong>splits a full episode into individual small episodes for every life that a player has</strong>. Not all games support this feature (for example, Pong doesn’t), but for the supported environments, it usually helps to speed up convergence as our episodes become shorter.</p></li>
<li><p>In the beginning of the game, performing a random amount (up to 30) of <strong>no-op actions</strong>. This should stabilize training, but there is no proper explanation why it is the case.</p></li>
<li><p>Making an action decision every K steps, where K is usually 4 or 3. On intermediate frames, the chosen action is simply repeated. This allows training to speed up significantly, as processing every frame with a neural network is quite a demanding operation, but the difference between consequent frames is usually minor.</p></li>
<li><p><strong>Taking the maximum of every pixel in the last two frames</strong> and using it as an observation. Some Atari games have a flickering effect, which is due to the platform’s limitation (Atari has a limited amount of sprites that can be shown on a single frame). For a human eye, such quick changes are not visible, but they can confuse neural networks.</p></li>
<li><p><strong>Pressing FIRE in the beginning of the game</strong>. Some games (including Pong and Breakout) require a user to press the FIRE button to start the game. In theory, it’s possible for a neural network to learn to press FIRE itself, but it will require much more episodes to be played. So, we press FIRE in the wrapper.</p></li>
<li><p><strong>Scaling every frame down</strong> from 210 x 160, with three color frames, <strong>into a single-color 84 x 84 image</strong>. Different approaches are possible. For example, the DeepMind paper describes this transformation as taking the Y-color channel from the YCbCr color space and then rescaling the full image to an 84 x 84 resolution. Some other researchers do grayscale transformation, cropping non-relevant parts of the image and then scaling down. In the Baselines repository (and in the following example code), the latter approach is used.</p></li>
<li><p><strong>Stacking several (usually 4) subsequent frames together</strong> to give the network the information about the dynamics of the game’s objects.</p></li>
<li><p><strong>Clipping the reward to -1, 0, and 1 values</strong> The obtained score can vary wildly among the games. For example, in Pong you get a score of 1 for every ball that your opponent passes behind you. However, in some games, like KungFu, you get a reward of 100 for every enemy killed. This spread in reward values makes our loss have completely different scales between the games, which makes it harder to find common hyperparameters for a set of games. To fix this, reward just gets clipped to the range [-1…1].</p></li>
<li><p><strong>Converting observations from unsigned bytes to float32 values</strong>. The screen obtained from the emulator is encoded as a tensor of bytes with values from 0 to 255, which is not the best representation for a neural network. So, we need to convert the image into floats and rescale the values to the range [0.0,1.0].</p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="license.html" class="btn btn-neutral float-right" title="License" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="algo/ppo.html" class="btn btn-neutral float-left" title="PPO" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Damien

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>