

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Introduction to RL &mdash; XRL  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Deep Q Network" href="dqn_intro.html" />
    <link rel="prev" title="Overview - Visualizing and Understanding Atari Agents" href="../biblio/notebook/Overview_Visualizing_Atari_Agents.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> XRL
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Bibliographie:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../biblio/visu_understanding_atari.html">Visualizing and Understanding Atari Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../biblio/notebook/Overview_Visualizing_Atari_Agents.html">Overview - Visualizing and Understanding Atari Agents</a></li>
</ul>
<p class="caption"><span class="caption-text">General Knowledge:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction to RL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#markov-decision-processes-mdps">Markov Decision Processes (MDPs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dynamic-programming">Dynamic Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-free-reinforcement-learning">Model-Free Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sarsa">SARSA</a></li>
<li class="toctree-l2"><a class="reference internal" href="#q-learning">Q-learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#actor-critic">Actor Critic</a></li>
<li class="toctree-l2"><a class="reference internal" href="#off-policy-rl-and-replay-buffer">Off-Policy RL and Replay Buffer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dqn_intro.html">Deep Q Network</a></li>
</ul>
<p class="caption"><span class="caption-text">Algorithms:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../algo/dqn.html">DQN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algo/ppo.html">PPO</a></li>
</ul>
<p class="caption"><span class="caption-text">Annexes:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../lexicon.html">Lexicon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../help.html">Help</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">XRL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Introduction to RL</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/rl/introduction.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="introduction-to-rl">
<h1>Introduction to RL<a class="headerlink" href="#introduction-to-rl" title="Permalink to this headline">¶</a></h1>
<p>In <strong>supervised learning</strong>, the supervisor indicates to the agent the <em>expected answer</em>. The agent can correct its model based on the answer via <em>gradient backpropagation, RLS</em>, … (either for regression or classification).
The <strong>exploration/exploitation</strong> trade-off : as exploiting can be harmful, shall I exploit what I know or should I look for a better policy ? Am I optimal, should I keep exploiting or stop ?
<span class="math notranslate nohighlight">\(\epsilon\)</span>-<strong>greedy</strong> : take the best action most of the time and a random action from time to time.</p>
<div class="section" id="markov-decision-processes-mdps">
<h2>Markov Decision Processes (MDPs)<a class="headerlink" href="#markov-decision-processes-mdps" title="Permalink to this headline">¶</a></h2>
<p>{2}</p>
<img alt="../_images/MDPs.png" src="../_images/MDPs.png" />
<ul class="simple">
<li><p>S : State space</p></li>
<li><p>A : Action space</p></li>
<li><p>T : <span class="math notranslate nohighlight">\(S\times A \to \pi(S)\)</span> : Transition function</p></li>
<li><p>r : <span class="math notranslate nohighlight">\(S \times A \to \mathbb{R}\)</span> : Reward Function</p></li>
</ul>
<p>An MDP <em>describes</em> a problem (it’s not a solution). It can be a <em>stochastic/deterministic</em> pb, with the generalized formulation : <span class="math notranslate nohighlight">\(T(s^t, a^t, s^{t+1}) = p(s'|s,a)\)</span>, (s,a) being the current state/action.Rewards can be over states or over action/state pairs.</p>
<p>The <strong>goal of the agent</strong> is to find the policy <span class="math notranslate nohighlight">\(\pi : S \times A\)</span> maximizing an aggregation of rewards on the long run.</p>
<p><strong>Theorem</strong> : for any MDP, it exists a deterministic policy that is optimal}</p>
<p><strong>Aggregation Criterion</strong> : computation of value functions assumes the choice of an aggregation criterion (<em>discounted, average,…</em>), and can be hard to compare as a sum over infinite horizon may be infinite.</p>
<ul class="simple">
<li><p><em>mere sum</em> (on finite horizon N) : <span class="math notranslate nohighlight">\(V^\pi (s_0) = r_0 + r_1 + ... + r_N\)</span></p></li>
<li><p><em>average on window k</em> : <span class="math notranslate nohighlight">\(V^\pi (s_0) = \frac{r_0 + r_1 + ... + r_k}{k+1}\)</span></p></li>
<li><p><em>discounted criterion</em>: <span class="math notranslate nohighlight">\(V^\pi (s_0) = \sum_{t=0}^{\inf} \gamma^t r(s_t, \pi(s_t))\)</span>, with <span class="math notranslate nohighlight">\(\gamma \in [0,1]\)</span></p></li>
</ul>
<p>Rmk : <span class="math notranslate nohighlight">\(\gamma \to 0\)</span> sensitive to immediate rewards, whereas <span class="math notranslate nohighlight">\(\gamma \to 1\)</span> future rewards are as important as immediate rewards.</p>
<p><strong>Markov Property</strong>:
<span class="math notranslate nohighlight">\(p(s^{t+1}|s^{t}, a^{t}) = p(s^{t+1}|s^{t}, a^{t}, s^{t-1}, a^{t-1}, ..., s^0, a^0)\)</span></p>
<p>An MDP defines <span class="math notranslate nohighlight">\(s_{t+1}, r_{t+1}\)</span> as <span class="math notranslate nohighlight">\(f(s_t, a_t)\)</span>. In an MDP, a <strong>memory of the past does not provide</strong> any useful advantage, via the following property. Reactive agents <span class="math notranslate nohighlight">\(a_{t+1} = f(s_t)\)</span> can be optimal. But this assumption is <strong>very strong</strong> and not verified in practice !</p>
</div>
<div class="section" id="dynamic-programming">
<h2>Dynamic Programming<a class="headerlink" href="#dynamic-programming" title="Permalink to this headline">¶</a></h2>
<p><strong>Value function</strong> : <span class="math notranslate nohighlight">\(V^{\pi} : S \to \mathbb{R}\)</span> aggregation of rewards on the long run for each state (by following the policy <span class="math notranslate nohighlight">\(\pi\)</span>. A vector with one entry per state.</p>
<p><strong>Action-Value Function</strong> : <span class="math notranslate nohighlight">\(Q^{\pi} : S\times A \to \mathbb{R}\)</span> : aggregation of rewards on the long run for doing each action in each state (and then following <span class="math notranslate nohighlight">\(\pi\)</span>). A matrix with one entry per (state, action).</p>
<p><strong>Bellman Equations</strong>:</p>
<ul class="simple">
<li><p><em>deterministic</em> : <span class="math notranslate nohighlight">\(V^{\pi}(s) = r(s, \pi(s)) + \gamma \sum_{s'} p (s'|s, \pi(s)) V^{\pi}(s')\)</span></p></li>
<li><p><em>stochastic</em> <span class="math notranslate nohighlight">\(\pi\)</span> : <span class="math notranslate nohighlight">\(V^{\pi}(s) = \sum_{a} \pi(s,a) \left[ r(s, a) + \gamma \sum_{s'} p (s'|s, a) V^{\pi}(s') \right]\)</span></p></li>
</ul>
<p>In dynamic programming, <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(r\)</span> are given. The agents knows everything about the transition function <span class="math notranslate nohighlight">\(T\)</span> and the reward function. From that knowledge, it can iteratively compute an optimal value function. In Reinforcement Learning, we don’t know T, nor <span class="math notranslate nohighlight">\(r\)</span> in advance, meaning we have to build the optimal policy <span class="math notranslate nohighlight">\(\pi^{*}\)</span> without T or <span class="math notranslate nohighlight">\(r\)</span>.</p>
</div>
<div class="section" id="model-free-reinforcement-learning">
<h2>Model-Free Reinforcement Learning<a class="headerlink" href="#model-free-reinforcement-learning" title="Permalink to this headline">¶</a></h2>
<p><strong>Model-Free approach</strong> : build <span class="math notranslate nohighlight">\(\pi^{*}\)</span> without estimating <span class="math notranslate nohighlight">\(T\)</span> or <span class="math notranslate nohighlight">\(r\)</span> (<em>actor critic, …</em>), opposed to <strong>model-based approach</strong> where we build a model of <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(r\)</span> and use it to improve the policy (via dynamic programming).</p>
<p><strong>Incremental Estimation</strong> / Immediate reward in state s :
<span class="math notranslate nohighlight">\(E_{k+1}(s) = E_k(s) + \alpha ( r_{k+1} - E_k(s))\)</span></p>
<p>This converges to the true average (slower of faster depending on <span class="math notranslate nohighlight">\(\alpha\)</span>) without storing anything (except for k and <span class="math notranslate nohighlight">\(E_k\)</span>).</p>
<p><strong>Temporal Difference Error</strong> : whose goal is to estimate the value function <span class="math notranslate nohighlight">\(V(s)\)</span>. If <span class="math notranslate nohighlight">\(V(s_t)\)</span> and <span class="math notranslate nohighlight">\(V(s_{t+1})\)</span> were exact, then we’d get <span class="math notranslate nohighlight">\(V(s_t) = r_t + \gamma V(s_{t+1})\)</span>. Hence, <span class="math notranslate nohighlight">\(\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\)</span> represents the error between <span class="math notranslate nohighlight">\(V(s_t)\)</span> and the value it should have given to <span class="math notranslate nohighlight">\(r_t\)</span>.</p>
<p>As a rmk, <span class="math notranslate nohighlight">\(\delta_t &gt; 0\)</span> means you underestimated the value of the current state.</p>
<p><strong>Policy Evaluation - TD(0)</strong> : given policy <span class="math notranslate nohighlight">\(\pi\)</span>, the agent perform a sequence <span class="math notranslate nohighlight">\(s_0, a_0, r_1, ...., s_t, a_t, r_{t+1}\)</span>.</p>
<p>At each step, <span class="math notranslate nohighlight">\(V(s_{t}) \longleftarrow V(s_{t}) + \alpha(r_{t+1} + \gamma V(s_{t+1}) - V(s_{t}))\)</span></p>
<p><strong>Limitations</strong> : Although TD(0) evaluates V(s), one can not infer <span class="math notranslate nohighlight">\(\pi(s)\)</span> from <span class="math notranslate nohighlight">\(V(0)\)</span> without knowing the transition function T, ie one must know which action <span class="math notranslate nohighlight">\(a\)</span> leads to the best <span class="math notranslate nohighlight">\(V(s)\)</span>.</p>
<p><strong>Solutions</strong> : (1) work with Q(s,a) (2) learn a model of T with model-based RL (3) opt for Action-Critic methods that update V and <span class="math notranslate nohighlight">\(\pi\)</span></p>
</div>
<div class="section" id="sarsa">
<h2>SARSA<a class="headerlink" href="#sarsa" title="Permalink to this headline">¶</a></h2>
<p><strong>SARSA</strong> - State Action Reward State Action :</p>
<p>For each observed (<span class="math notranslate nohighlight">\(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\)</span>) :</p>
<p><span class="math notranslate nohighlight">\(Q(s_t, a_t) \longleftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]\)</span></p>
<p>SARSA is an <strong>on-policy</strong> method, as you have to decide what action <span class="math notranslate nohighlight">\(a_{t+1}\)</span> to do in the next time step.</p>
</div>
<div class="section" id="q-learning">
<h2>Q-learning<a class="headerlink" href="#q-learning" title="Permalink to this headline">¶</a></h2>
<p><strong>Q-learning</strong>  :</p>
<p>For each observed (<span class="math notranslate nohighlight">\(s_t, a_t, r_{t+1}, s_{t+1}\)</span>) :</p>
<p><span class="math notranslate nohighlight">\(Q(s_t, a_t) \longleftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max\limits_{a \in A} Q(s_{t+1}, a) - Q(s_t, a_t) \right]\)</span></p>
<p>Q-learning is an <strong>off-policy</strong> method, as it takes the best action in the next step.</p>
<p><strong>Q-learning</strong> = learn the <em>action-value</em> function <span class="math notranslate nohighlight">\(Q(s,a)\)</span>. How good it is to perform a particular action in a particular state.</p>
<p>In <em>discrete space</em>, one builds a memory table / a <strong>Q-tables</strong> Q[s,a] to state Q-values for all possible combination of <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(a\)</span>. “<em>Q-learning is about creating the cheat sheet Q</em>”.</p>
<p><strong>Q-learning Algorithm</strong>  :</p>
<ul class="simple">
<li><p>Initialisation : start with <span class="math notranslate nohighlight">\(Q_0(s,a)\)</span> for all s,a. Get initial state <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p>For k=1,.., till CV :</p></li>
<li><p>Sample action <span class="math notranslate nohighlight">\(a\)</span> and get next state <span class="math notranslate nohighlight">\(s'\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(s'\)</span> is terminal : target = <span class="math notranslate nohighlight">\(R(s,a,s')\)</span>, sample new initial state <span class="math notranslate nohighlight">\(s\)</span></p></li>
</ul>
<p>Else, target = <span class="math notranslate nohighlight">\(R(s,a,s') + \gamma \max\limits_{a'} Q_k(s', a')\)</span>
-  <span class="math notranslate nohighlight">\(Q_{k+1}(s', a') = (1-\alpha)Q_k(s, a) + \alpha \times \mbox{target}\)</span>.
<span class="math notranslate nohighlight">\(s \longleftarrow s'\)</span></p>
<p>Both <strong>SARSA and Q-learning have proved convergence</strong> given <span class="math notranslate nohighlight">\(\infty\)</span> exploration.</p>
<p>Q-learning in practice consists in building a table (states x actions) - Q-table - where you apply the update equation after each action. <strong>Problem</strong> : it is very slow, and if states and action states are <em>too large</em>, the memory of Q explodes. One solution would be to use a deep-network (DQN) to approximate Q(s,a).</p>
</div>
<div class="section" id="actor-critic">
<h2>Actor Critic<a class="headerlink" href="#actor-critic" title="Permalink to this headline">¶</a></h2>
<p><strong>Idea</strong> : learn the value function in addition to the policy, using two models : a <em>critic</em> that updates the value function parameter (can be either <span class="math notranslate nohighlight">\(Q_w(a|s)\)</span> or <span class="math notranslate nohighlight">\(V_w(s)\)</span>) and a <em>critic</em> that updates the policy parameter <span class="math notranslate nohighlight">\(\theta\)</span> for <span class="math notranslate nohighlight">\(\pi_\theta(a|s)\)</span> in the direction suggested by the critic.</p>
<p><strong>Actor-Critic</strong>  :
{2}</p>
<p>The agent manages two representations at the same time :</p>
<ul class="simple">
<li><p><strong>a critic</strong>, updated with TD from state and reward. It computes the TD error <span class="math notranslate nohighlight">\(\delta_k\)</span> and update <span class="math notranslate nohighlight">\(V_k(s) \longleftarrow V_k(s) + \alpha_k\delta_k\)</span></p></li>
<li><p><strong>an actor</strong>, updated depending on the critic’s update. <span class="math notranslate nohighlight">\(P^{\pi}(a|s) = P^{\pi}(a|s) + \alpha_k' \delta_k\)</span></p></li>
</ul>
<img alt="../_images/AC.png" src="../_images/AC.png" />
<p>Note that there are two learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> (critic) and <span class="math notranslate nohighlight">\(\alpha^{'}\)</span> (actor)</p>
<p>Quick illustration : if you compute the TD error and find a <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span>, from a state <span class="math notranslate nohighlight">\(s\)</span> by taking action <span class="math notranslate nohighlight">\(a\)</span>. It means the value of the state is higher that you thought, hence the action that you just performed is better than what you thought, thus you need to increase the probability of taking that action.</p>
<p><strong>Simple action-value AC algorithm</strong>  :</p>
<ul class="simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(s, \theta, w\)</span> at random</p></li>
</ul>
<p>Sample <span class="math notranslate nohighlight">\(a \sim \pi_\theta(a|s)\)</span>
-  for t = 1, .., T do :</p>
<ul class="simple">
<li><p>sample reward <span class="math notranslate nohighlight">\(r_t \sim R(s,a)\)</span> and get next state <span class="math notranslate nohighlight">\(s' \sim P'(s'|s,a)\)</span></p></li>
<li><p>sample next action <span class="math notranslate nohighlight">\(a' \sim \pi_\theta(a'| s')\)</span></p></li>
<li><p>update policy parameter <span class="math notranslate nohighlight">\(\theta\)</span> using</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\theta \longleftarrow \theta + \alpha_\theta Q_w(s,a) \nabla_\theta \mbox{ ln } \pi_\theta(a|s)\)</span>
-  Compute TD error (correction) for action-value at time t :
<span class="math notranslate nohighlight">\(\delta_t = r_t + \gamma Q_w(s', a') - Q_w(s,a)\)</span>
and use it to update the parameter of the action-value function :
<span class="math notranslate nohighlight">\(w \longleftarrow w + \alpha_w \delta_t \nabla_w Q_w(s,a)\)</span>
-  update <span class="math notranslate nohighlight">\(a \longleftarrow a', s \longleftarrow s'\)</span></p>
<p>Remark : two learning rates <span class="math notranslate nohighlight">\(\alpha_\theta, \alpha_w\)</span></p>
</div>
<div class="section" id="off-policy-rl-and-replay-buffer">
<h2>Off-Policy RL and Replay Buffer<a class="headerlink" href="#off-policy-rl-and-replay-buffer" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="dqn_intro.html" class="btn btn-neutral float-right" title="Deep Q Network" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../biblio/notebook/Overview_Visualizing_Atari_Agents.html" class="btn btn-neutral float-left" title="Overview - Visualizing and Understanding Atari Agents" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Damien

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>