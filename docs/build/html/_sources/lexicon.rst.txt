Lexicon
=======

Actor-Critic
^^^^^^^^^^^^

When attempting to solve a Reinforcement Learning problem, there are two main methods one can choose from: calculating the Value Functions or Q-Values of each state and choosing actions according to those, or directly compute a policy which defines the probabilities each action should be taken depending on the current state, and act according to it. Actor-Critic algorithms combine the two methods in order to create a more robust method. A great illustrated-comics explanation can be found here.
https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752

 
Cross-Entropy (Loss Function)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events. It builds upon the idea of entropy from information theory and calculates the number of bits required to represent or transmit an average event from one distribution compared to another distribution.

**Intuition** :  if we consider a target or underlying probability distribution :math:`P` and an approximation of the target distribution :math:`Q`, then the *cross-entropy* of :math:`Q` from :math:`P` is the number of additional bits to represent an event using :math:`Q` instead of :math:`P` : 

:math:`H(P, Q) = - \sum_{x \in X} P(x)\log (Q(x))`

Distillation
^^^^^^^^^^^^

Method to transfer knowledge from a *teacher model T* to a *student model S*.

It is typically used for model compression, in which a small model is trained to mimic a pre-trained model (or ensemble of models). This training setting is referred to as *teacher-student*.

**Paper** : introduced in \href{https://www.cs.cornell.edu/\~caruana/compression.kdd06.pdf}{*Model Compression*}, C. Bucila (2006) and generalized in \href{https://arxiv.org/abs/1503.02531}{*Distilling the Knowledge in a Neural Network*}, C. Hinton. Other materials : \href{https://www.youtube.com/watch?v=EK61htlw8hY}{video lecture} and \href{https://www.ttic.edu/dl/dark14.pdf}{slides}.

**How does it work** : knowledge is transferred from the teacher model to the student by minimizing a loss function in which the target is the distribution of class probabilities predicted by the teacher model, ie the output of a softmax function on the teacher model's logits. 

**Sofmax's temperature :math:`\tau`** : in many cases, the probability distribution has the correct class at a very high probability, with all other class probabilities very close to 0, which doesn't provide much information beyond the ground truth labels already provided in the dataset. To tackle this issue, Hinton et al., 2015 introduced the concept of "softmax temperature". With T as the temperature parameter, the probability :math:`p_i` of class i is calculated from the logits z as:
:math:`p_i = \frac{exp(\frac{z_i}{T})}{\sum_j exp(\frac{z_j}{T})}`


-  When T=1 we get the standard softmax function.
-  As T grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes the teacher found more similar to the predicted class

Remarks : 

-  *distillation loss* : when computing the loss function vs. the teacher's soft targets, we use the same value of T to compute the softmax on the student's logits.
-  *student loss* : calculate the "standard" loss between the student's predicted class probabilities and the ground-truth labels (also called "hard labels/targets") - and using :math:`T=1`.
-  The overall loss function, incorporating both distillation and student losses, is calculated as:
:math:`L(x,W) = \alpha H(y, \sigma(z_s, T=1)) + \beta H(\sigma(z_t, T=\tau), \sigma(z_s, T=\tau))`, with x the input, W the student model parameter, y the ground truth label, H the cross entropy loss function, :math:`\sigma` the softmax function, :math:`z_s, z_t` respectively the logits of students and teacher.


.. image:: Images/knowledge_distillation.png

Read more on \href{https://nervanasystems.github.io/distiller/knowledge_distillation.html#bucila-et-al-2006}{Knowledge Distillation
}

Exploration vs Exploitation trade-off
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

On the one hand, our agent needs to explore the environment to build a complete picture of transitions and action outcomes. On the other hand, we should use interaction with the environment efficiently: we shouldn't waste time by randomly trying actions we've already tried and have learned their outcomes.

Model-Free vs Model-Based
^^^^^^^^^^^^^^^^^^^^^^^^^

The term **model-free** means that the method doesn't build a model of the environment or reward; it just directly connects observations to actions (or values that are related to actions). In other words, the agent takes current observations and does some computations on them, and the result is the action that it should take.

In contrast, **model-based** methods try to predict what the next
observation and/or reward will be. Based on this prediction, the agent is trying to
choose the best possible action to take, very often making such predictions
multiple times to look more and more steps into the future.

Both classes of methods have strong and weak sides, but usually pure model-based methods are used in deterministic environments, such as board games with
strict rules. On the other hand, model-free methods are usually easier to train as
it's hard to build good models of complex environments with rich observations.

Motor Learning
^^^^^^^^^^^^^^

often regarded as a process of learning a new mapping from a sensory inputs to motor outputs

Off Policy vs On Policy
^^^^^^^^^^^^^^^^^^^^^^^

Every Reinforcement Learning algorithm must follow some policy in order to decide which actions to perform at each state. Still, the learning procedure of the algorithm doesn't have to take into account that policy while learning. Algorithms which concern about the policy which yielded past state-action decisions are referred to as on-policy algorithms, while those ignoring it are known as off-policy.
A well known off-policy algorithm is Q-Learning, as its update rule uses the action which will yield the highest Q-Value, while the actual policy used might restrict that action or choose another. The on-policy variation of Q-Learning is known as Sarsa, where the update rule uses the action chosen by the followed policy

Policy-based vs Value-based
^^^^^^^^^^^^^^^^^^^^^^^^^^^

**Policy-based** methods are directly approximating the policy of the agent, that is, what actions the agent should
carry out at every step. Policy is usually represented by probability distribution over the available actions. 

In contrast, the method could be **value-based**. In this
case, instead of the probability of actions, the agent calculates the value of every possible action and chooses the action with the best value.


Replay Buffer / Memory 
^^^^^^^^^^^^^^^^^^^^^^^

It stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure.

Tabular Q-learning
^^^^^^^^^^^^^^^^^^


-  Start with an empty table :math:`Q(s,a)`, mapping states to values of actions.
-  By interacting with the environment, obtain the tuple (s, a, r, s') (state, action,reward, and the new state). Need to decide which action to take, and there is no single proper way to make this decision (exploration versus exploitation).
-  Update the Q(s, a) value using the Bellman approximation update: :math:`Q_{(s,a)} \longleftarrow r + \gamma \max_{a' \in A} Q_{(s',a')}`
-  Repeat from step 2 til convergence.

Remark, to make the training more stable, use the *blending* technique which is just averaging between old and new values of Q using learning rate :math:`\alpha`. This allows values of Q to converge smoothly, even if our environment is noisy :

:math:`Q_{(s,a)} \longleftarrow (1-\alpha) Q_{(s,a)} + \alpha \left(r + \gamma \max_{a' \in A} Q_{(s',a')} \right)`

Target Clipping
^^^^^^^^^^^^^^^

Mitigate target critic overestimation

Target Network
^^^^^^^^^^^^^^

The target network has its weights kept frozen most of the time, but is updated with the policy network's weights every so often. This is usually a set number of steps but we shall use episodes for simplicity.The target network has its weights kept frozen most of the time, but is updated with the policy network's weights every so often. This is usually a set number of steps but we shall use episodes for simplicity.


Value Iteration
^^^^^^^^^^^^^^^

A method that on every step does a loop on all states, and for every state, performs an update of its value with a Bellman approximation.

(Atari) Wrappers
^^^^^^^^^^^^^^^^

The list of Atari transformations used by RL researchers includes:

-  **Converting individual lives in the game into separate episodes**. In general, an episode contains all the steps from the beginning of the game until the "Game over" screen appears?, which can last for thousands of game steps (observations and actions). Usually, in arcade games, the player is given several lives, which provide several attempts in the game. This transformation **splits a full episode into individual small episodes for every life that a player has**. Not all games support this feature (for example, Pong doesn't), but for the supported environments, it usually helps to speed up convergence as our episodes become shorter. 
-  In the beginning of the game, performing a random amount (up to 30) of **no-op actions**. This should stabilize training, but there is no proper explanation why it is the case.
-  Making an action decision every K steps, where K is usually 4 or 3. On intermediate frames, the chosen action is simply repeated. This allows training to speed up significantly, as processing every frame with a neural network is quite a demanding operation, but the difference between consequent frames is usually minor.
-  **Taking the maximum of every pixel in the last two frames** and using it as an observation. Some Atari games have a flickering effect, which is due to the platform's limitation (Atari has a limited amount of sprites that can be shown on a single frame). For a human eye, such quick changes are not visible, but they can confuse neural networks.
-  **Pressing FIRE in the beginning of the game**. Some games (including Pong and Breakout) require a user to press the FIRE button to start the game. In theory, it's possible for a neural network to learn to press FIRE itself, but it will require much more episodes to be played. So, we press FIRE in the wrapper.

-  **Scaling every frame down** from 210 x 160, with three color frames, **into a single-color 84 x 84 image**. Different approaches are possible. For example, the DeepMind paper describes this transformation as taking the Y-color channel from the YCbCr color space and then rescaling the full image to an 84 x 84 resolution. Some other researchers do grayscale transformation, cropping non-relevant parts of the image and then scaling down. In the Baselines repository (and in the following example code), the latter approach is used.

-  **Stacking several (usually 4) subsequent frames together** to give the network the information about the dynamics of the game's objects.
-  **Clipping the reward to -1, 0, and 1 values** The obtained score can vary wildly among the games. For example, in Pong you get a score of 1 for every ball that your opponent passes behind you. However, in some games, like KungFu, you get a reward of 100 for every enemy killed. This spread in reward values makes our loss have completely different scales between the games, which makes it harder to find common hyperparameters for a set of games. To fix this, reward just gets clipped to the range [-1...1].
-  **Converting observations from unsigned bytes to float32 values**. The screen obtained from the emulator is encoded as a tensor of bytes with values from 0 to 255, which is not the best representation for a neural network. So, we need to convert the image into floats and rescale the values to the range [0.0,1.0].
